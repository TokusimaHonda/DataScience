{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tweet_analyze_sentiment_api.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP6RttJ/v83qL8zHtTDlHAQ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBYEccOC47Gl"
      },
      "source": [
        "import json\n",
        "from requests_oauthlib import OAuth1Session\n",
        "import pandas\n",
        "import requests\n",
        "pandas.set_option(\"display.max_colwidth\", 200)\n",
        "import numpy"
      ],
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3q6IOQGb6Wsa"
      },
      "source": [
        "CONSUMER_KEY = ''\n",
        "CONSUMER_SECRET = ''\n",
        "ACCESS_TOKEN = ''\n",
        "ACCESS_TOKEN_SECRET = ''\n",
        "twitter = OAuth1Session(CONSUMER_KEY,CONSUMER_SECRET,ACCESS_TOKEN,ACCESS_TOKEN_SECRET)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNtIxF55WXEv"
      },
      "source": [
        "GOOGLE_API_KEY = ''\n"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmWL0gjj70ja"
      },
      "source": [
        "def analyze_sentiment(analyze_text):\n",
        "    url = f'https://language.googleapis.com/v1beta2/documents:analyzeSentiment?key={GOOGLE_API_KEY}'\n",
        "    header = {'Content-Type':'application/json'}\n",
        "    body = {\n",
        "        \"document\":{\n",
        "            \"type\" : \"PLAIN_TEXT\",\n",
        "            \"language\" : \"JA\",\n",
        "            \"content\" : analyze_text\n",
        "        }\n",
        "    }\n",
        "    res = requests.post(url, headers=header,json=body)\n",
        "    if (res.status_code == 200):\n",
        "            result_analyze_sentiment = json.loads(res.text)\n",
        "            return result_analyze_sentiment"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKCpov6uYqIn"
      },
      "source": [
        ""
      ],
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0utZD7NF8Aj5"
      },
      "source": [
        "# 特定ユーザーのツイートを取得\n",
        "def get_user_tweet_dataframe(search_twitter_id):\n",
        "    url = 'https://api.twitter.com/1.1/statuses/user_timeline.json'\n",
        "    #最大2200件のツイートを取得するためのページ\n",
        "    pages = [1,2,3,4,5,6,7,8,9,10,11]\n",
        "    df_timelines_concat = pandas.DataFrame()\n",
        "    for page in pages:\n",
        "\n",
        "        params = {'screen_name':search_twitter_id,\n",
        "                'count':200,\n",
        "                'page':page,\n",
        "                'include_entities' : True,\n",
        "                'exclude_replies' : False,\n",
        "                'include_rts' : False\n",
        "                }\n",
        "        res = twitter.get(url,params=params)\n",
        "        if (res.status_code == 200):\n",
        "            timelines = json.loads(res.text)\n",
        "            df_timelines = pandas.DataFrame(timelines)\n",
        "            df_timelines_concat = pandas.concat([df_timelines_concat,df_timelines],axis=0)\n",
        "        \n",
        "    return df_timelines_concat"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CV23ed6eGosJ"
      },
      "source": [
        "# 特定のユーザーへのリプライのみ取得\n",
        "def get_user_replies_tweet_dataframe(search_twitter_id,search_replies_twitter_id_list=[]):\n",
        "    df_tweet = get_user_tweet_dataframe(search_twitter_id)\n",
        "    df_reply_tweet = df_tweet.dropna(subset=['in_reply_to_screen_name'])\n",
        "    if (len(search_replies_twitter_id_list)>0):\n",
        "        df_reply_user_tweet = df_reply_tweet[df_reply_tweet['in_reply_to_screen_name'].isin(search_replies_twitter_id_list)].copy()\n",
        "        for search_replies_twitter_id in search_replies_twitter_id_list:\n",
        "            df_reply_user_tweet.text = df_reply_user_tweet.text.replace(f'@{search_replies_twitter_id}', '', regex=True)\n",
        "        return df_reply_user_tweet\n",
        "    \n",
        "    else:\n",
        "        return df_reply_tweet"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIbfR1iR-UiC"
      },
      "source": [
        "search_twitter_id = 'twitter_account_id'\n",
        "search_replies_twitter_id_list = ['twitter_account_id']\n",
        "df_tweet = get_user_replies_tweet_dataframe(search_twitter_id,search_replies_twitter_id_list)[['created_at','text']].copy()"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssnTkmIxd5Jk"
      },
      "source": [
        "# 取得したツイートを感情分析\n",
        "analyze_sentiment_list = []\n",
        "for tweet_text in df_tweet['text'].values:\n",
        "    analyze_sentiment_list.append(analyze_sentiment(tweet_text))"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5Fd9fgAfrcO"
      },
      "source": [
        "#感情分析結果をdataframeにセット\n",
        "df_analyze_sentiment = pandas.DataFrame(analyze_sentiment_list)"
      ],
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXR6RrnIoCYj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bU3M7_mT-kWi"
      },
      "source": [
        ""
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bV8M0e4h-rA7"
      },
      "source": [
        "df_analyze_sentiment['magnitude'] = 0.00\n",
        "df_analyze_sentiment['score'] = 0.00\n",
        "\n",
        "# 文章感情分析のスコアを列にセット\n",
        "for index, row in df_analyze_sentiment.iterrows():\n",
        "    df_analyze_sentiment.at[index, 'magnitude'] += row['documentSentiment']['magnitude']\n",
        "    df_analyze_sentiment.at[index, 'score'] += row['documentSentiment']['score']\n",
        "\n",
        "df_analyze_sentiment['magnitude_score'] = (df_analyze_sentiment['magnitude'] * df_analyze_sentiment['score']).abs()"
      ],
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHc6Upqh-sqi"
      },
      "source": [
        ""
      ],
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZS3yL01-xba"
      },
      "source": [
        ""
      ],
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqAmkrFT_FVD"
      },
      "source": [
        "# 感情分析をセンテンス毎に分解\n",
        "shaping_sentences_sentiment_dic_list = []\n",
        "for sentences_sentiment_list in df_analyze_sentiment['sentences'].values:\n",
        "    for sentences_sentiment_dic in sentences_sentiment_list:\n",
        "        shaping_sentences_sentiment_dic = {'content':sentences_sentiment_dic['text']['content'],\n",
        "                                                                'magnitude':sentences_sentiment_dic['sentiment']['magnitude'],\n",
        "                                                                'score':sentences_sentiment_dic['sentiment']['score']\n",
        "                                                            }\n",
        "        shaping_sentences_sentiment_dic_list.append(shaping_sentences_sentiment_dic)\n",
        "df_analyze_sentences_sentiment = pandas.DataFrame(shaping_sentences_sentiment_dic_list)"
      ],
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Af69SuPapr7D"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}